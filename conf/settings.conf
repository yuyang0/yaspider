user_agent  Yaspiser

max_conns  100

max_ram_urls  10000

max_site_urls  50

# Max number of dns calls in parallel 
max_dns_calls  10

# How deep do you want to go in a site
site_depth  5

#default time out
connection_timeout  30

# time between 2 calls on the same server (in sec)  NEVER less than 30
wait_duration  15

# Make requests through a proxy (use with care)
#proxy www 8080

##############################################
# now, let's customize the search

# first page to fetch (you can specify several urls)
start_urls  {
    http://www.oschina.net
    http://www.douban.com
    http://www.w3c.com
}

#if running daemon (on/off)
daemon  on

allowed_exts  {
   .pdf .png .jpg .jpeg .bmp .smi .tiff .gif
   .html .htm 
}
prior_exts  {
    .html .htm
}
forb_exts  {
    .tar.gz .tar.bz2 .tar .zip .Z .rar .zip .7z .deb .rpm .exe
    .ps .dvi .pdf .doc .xls .ppt .rtf .psd .mdb
    .png .jpg .jpeg .bmp .smi .tiff .gif
    .mov .avi .mpeg .mpg .mp3 .mp4 .qt .wav .ram .rm .rmvb
    .jar .java .class .diff
}
# Do you want to limit your search to a specific domain ?
# if yes, uncomment the following lines

#allowed_domain {
#     .com .com.cn .cn
#      douban.com
#      oschina.net
# }

# urls belong to priority domain will be downloaded first
prior_domain  {
    douban.com
    zhihu.com
}
#how to save the page,the values allowed as fllows:
# 0: simple save, format: XXXXXXXX/0, XXXXXXXX/1 ......
# 1: save as a mirror of a site,it will make recursively folds according
#    to url's path
# 2: save in database(mysql only)
save 1
# the path to save the files we download
# it must be a directory and end with '/'
save_path    /home/yangyu/tmp/sites/

# have a server
have_server  on

server_port  11111
